{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text allignments (just example at this moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(501153, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('models/LaBSE-tuned/final')\n",
    "model = AutoModel.from_pretrained('models/LaBSE-tuned/final')\n",
    "model = model.to(\"cuda:1\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text piece was taken from: https://rus4all.ru/lez/20190528/11021/Sneg.html\n",
    "\n",
    "lez_text = \\\n",
    "\"\"\"\n",
    "ЧIехи баде дакIардин кIане пичинин патавай чилел вегьенвай месел алай. Адан виш йис хьанвай. ЧIехи яру цуьквер алай читдин чин алай яргъандикай хкатна аквазвайди адан гъвечIи хъхьанвай чин ва кьуранвай кьве гъил тир. Амма адаз килигай касдин виле сифтени-сифте акьазвайди дерин ва кьадар авачир биришар тир. Ам вичин биришра аруш хьанвай хьиз аквадай. Адан вилера экв амачир. ЧIехи бадедин кьилел чIулав ягълух, адан кIаникайни рехи шуткьу алай. Гададиз чидай хьи, чIехи бадедин кьилел чIар алач, ада кьве югъ идалай вилик вичин кьил гададин дидедив мукIратIдивди кьаз тунай, бадеди, лагьайтIа, яни гъвечIи бадеди чIехи бубадин уьлгуьчдалди аламукьай чIарар хтун хъувунай. Гададиз мадни чидай хьи, чIехи бадеди шуткьу кьилел чIарар аламачирди чуьнуьхун патал ваъ, вичин къедда авай дишегьлидал шуткьу тахьун айиб тирвиляй алукIзавай. Жив къвадалди ам са гьилле хъсанзавайди тир, мецел-ванцел чан аламай, махар ахъайдай. Гададиз адан вири махар, риваятар, негъилар, кьисаяр, бязи маниярни хуралай чидай, ятIани, гьар гъилера чIехи бадеди таниш за затI ахъайдайла, са цIийи лишан, цIийи везин, са цIийи гаф алава хъжедай. «Яраб идан рикIе икьван ксар, девирар, вакъиаяр, гафар гьикI гьакьзаватIа?» – фикирардай гадади. Садра ада бадедиз и суал ганай. «Ам Аллагьдин пай я, чан бала, адаз ганвай». Гада и гафарин гъавурда бегьемдиз акьуначир, амма чIехи баде амай инсанриз ухшар туширди, кьетIенди тирди гадади аннамишнай.\n",
    "\"\"\"\n",
    "\n",
    "ru_text = \\\n",
    "\"\"\"\n",
    "Прабабушка лежала в постели под окном на полу, ближе к печи. Ей было сто лет. Из-под шерстяного одеяла с ситцевым покрывалом в крупных красных цветах выглядывали лишь ее уменьшившееся лицо и высохшие руки. Прежде всего в глаза бросались глубокие бесчисленные морщины. Можно было подумать, что она запуталась в собственных морщинах. В ее глазах не осталось света. Голова была повязана черным платком, а под ним было серое шутку*. Мальчик знал, что у нее на голове нет волос: два дня назад она попросила мать мальчика ножницами остричь ей волосы, бабушка же младшая опасной бритвой дедушки сбрила то, что еще оставалось от волос. Еще мальчик знал: прабабушка надевала шутку не потому, что стеснялась оголенной головы, а потому, что женщине, соблюдающей горские приличия, не пристало быть без шутку. До того, как выпал снег, она еще чувствовала себя неплохо, рассказывала сказки, голос у нее был еще живой. Мальчик помнил все ее сказки, легенды, были и небылицы, даже песни, но каждый раз, когда она вновь начинала рассказывать уже знакомое, он замечал что-то новое, какой-либо образ, черточку или незнакомое слово. Мальчик думал: «Как в ее памяти умещается столько людей, эпох, событий и слов?» Однажды он задал этот вопрос младшей бабушке. «Всевышний наделил ее этим даром, сынок». Мальчик не совсем понял ­услышанное, но он осознал, что прабабушка не похожа на других людей, что она особенная.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for multilingual-e5-large\n",
    "# lez_sentences = [f\"query: {s.strip()}\" for s in lez_text.split(\".\")]\n",
    "# ru_sentences = [f\"query: {s.strip()}\" for s in ru_text.split(\".\")]\n",
    "\n",
    "# for LaBSE you shouldn't use prefix \"query: \"\n",
    "lez_sentences = [s.strip() for s in lez_text.split(\".\")]\n",
    "ru_sentences = [s.strip() for s in ru_text.split(\".\")]\n",
    "\n",
    "len(lez_sentences), len(ru_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(lez_sentences, max_length=512, padding=True, truncation=True, return_tensors='pt').to(\"cuda:1\")\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "lez_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "lez_embeddings = F.normalize(lez_embeddings, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(ru_sentences, max_length=512, padding=True, truncation=True, return_tensors='pt').to(\"cuda:1\")\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "ru_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "ru_embeddings = F.normalize(ru_embeddings, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(ru_embeddings.detach().cpu().numpy(), lez_embeddings.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: (0, 0), Cos Sim: 0.7824299931526184\n",
      "ru: Прабабушка лежала в постели под окном на полу, ближе к печи\n",
      "lez: ЧIехи баде дакIардин кIане пичинин патавай чилел вегьенвай месел алай\n",
      "Pair: (1, 1), Cos Sim: 0.9530683755874634\n",
      "ru: Ей было сто лет\n",
      "lez: Адан виш йис хьанвай\n",
      "Pair: (2, 2), Cos Sim: 0.7018488645553589\n",
      "ru: Из-под шерстяного одеяла с ситцевым покрывалом в крупных красных цветах выглядывали лишь ее уменьшившееся лицо и высохшие руки\n",
      "lez: ЧIехи яру цуьквер алай читдин чин алай яргъандикай хкатна аквазвайди адан гъвечIи хъхьанвай чин ва кьуранвай кьве гъил тир\n",
      "Pair: (3, 3), Cos Sim: 0.6578536033630371\n",
      "ru: Прежде всего в глаза бросались глубокие бесчисленные морщины\n",
      "lez: Амма адаз килигай касдин виле сифтени-сифте акьазвайди дерин ва кьадар авачир биришар тир\n",
      "Pair: (4, 4), Cos Sim: 0.7087996006011963\n",
      "ru: Можно было подумать, что она запуталась в собственных морщинах\n",
      "lez: Ам вичин биришра аруш хьанвай хьиз аквадай\n",
      "Pair: (5, 5), Cos Sim: 0.9358047246932983\n",
      "ru: В ее глазах не осталось света\n",
      "lez: Адан вилера экв амачир\n",
      "Pair: (6, 6), Cos Sim: 0.5600435137748718\n",
      "ru: Голова была повязана черным платком, а под ним было серое шутку*\n",
      "lez: ЧIехи бадедин кьилел чIулав ягълух, адан кIаникайни рехи шуткьу алай\n",
      "Pair: (7, 7), Cos Sim: 0.9187813997268677\n",
      "ru: Мальчик знал, что у нее на голове нет волос: два дня назад она попросила мать мальчика ножницами остричь ей волосы, бабушка же младшая опасной бритвой дедушки сбрила то, что еще оставалось от волос\n",
      "lez: Гададиз чидай хьи, чIехи бадедин кьилел чIар алач, ада кьве югъ идалай вилик вичин кьил гададин дидедив мукIратIдивди кьаз тунай, бадеди, лагьайтIа, яни гъвечIи бадеди чIехи бубадин уьлгуьчдалди аламукьай чIарар хтун хъувунай\n",
      "Pair: (8, 8), Cos Sim: 0.8286067843437195\n",
      "ru: Еще мальчик знал: прабабушка надевала шутку не потому, что стеснялась оголенной головы, а потому, что женщине, соблюдающей горские приличия, не пристало быть без шутку\n",
      "lez: Гададиз мадни чидай хьи, чIехи бадеди шуткьу кьилел чIарар аламачирди чуьнуьхун патал ваъ, вичин къедда авай дишегьлидал шуткьу тахьун айиб тирвиляй алукIзавай\n",
      "Pair: (9, 9), Cos Sim: 0.7299082279205322\n",
      "ru: До того, как выпал снег, она еще чувствовала себя неплохо, рассказывала сказки, голос у нее был еще живой\n",
      "lez: Жив къвадалди ам са гьилле хъсанзавайди тир, мецел-ванцел чан аламай, махар ахъайдай\n",
      "Pair: (10, 10), Cos Sim: 0.8381354808807373\n",
      "ru: Мальчик помнил все ее сказки, легенды, были и небылицы, даже песни, но каждый раз, когда она вновь начинала рассказывать уже знакомое, он замечал что-то новое, какой-либо образ, черточку или незнакомое слово\n",
      "lez: Гададиз адан вири махар, риваятар, негъилар, кьисаяр, бязи маниярни хуралай чидай, ятIани, гьар гъилера чIехи бадеди таниш за затI ахъайдайла, са цIийи лишан, цIийи везин, са цIийи гаф алава хъжедай\n",
      "Pair: (11, 11), Cos Sim: 0.7255071401596069\n",
      "ru: Мальчик думал: «Как в ее памяти умещается столько людей, эпох, событий и слов?» Однажды он задал этот вопрос младшей бабушке\n",
      "lez: «Яраб идан рикIе икьван ксар, девирар, вакъиаяр, гафар гьикI гьакьзаватIа?» – фикирардай гадади\n",
      "Pair: (12, 13), Cos Sim: 0.8035333752632141\n",
      "ru: «Всевышний наделил ее этим даром, сынок»\n",
      "lez: «Ам Аллагьдин пай я, чан бала, адаз ганвай»\n",
      "Pair: (13, 14), Cos Sim: 0.8068109750747681\n",
      "ru: Мальчик не совсем понял ­услышанное, но он осознал, что прабабушка не похожа на других людей, что она особенная\n",
      "lez: Гада и гафарин гъавурда бегьемдиз акьуначир, амма чIехи баде амай инсанриз ухшар туширди, кьетIенди тирди гадади аннамишнай\n",
      "Pair: (14, 15), Cos Sim: 1.0000001192092896\n",
      "ru: \n",
      "lez: \n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Hungarian algorithm: Solves linear sum assignment problem\n",
    "# https://en.wikipedia.org/wiki/Assignment_problem\n",
    "# This algorithms guarantees that a bipartitle matching will be found, where:\n",
    "# Each sentence from the first set will be matched to exactly one sentence from the second (1:1).  \n",
    "# And vice versa.\n",
    "\n",
    "\n",
    "row_ids, col_ids = linear_sum_assignment(1 - cos_sim)\n",
    "\n",
    "for i, j in zip(row_ids, col_ids):\n",
    "    print(f\"Pair: ({i}, {j}), Cos Sim: {cos_sim[i, j]}\")\n",
    "    print(f\"ru: {ru_sentences[i]}\")\n",
    "    print(f\"lez: {lez_sentences[j]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
